{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#하이퍼파라미터-세팅-및-seed-고정\" data-toc-modified-id=\"하이퍼파라미터-세팅-및-seed-고정-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>하이퍼파라미터 세팅 및 seed 고정</a></span></li><li><span><a href=\"#학습-데이터-EDA\" data-toc-modified-id=\"학습-데이터-EDA-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>학습 데이터 EDA</a></span></li><li><span><a href=\"#데이터-전처리-함수-정의-(Dataset)\" data-toc-modified-id=\"데이터-전처리-함수-정의-(Dataset)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>데이터 전처리 함수 정의 (Dataset)</a></span></li><li><span><a href=\"#Dataset-정의-및-DataLoader-할당\" data-toc-modified-id=\"Dataset-정의-및-DataLoader-할당-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dataset 정의 및 DataLoader 할당</a></span><ul class=\"toc-item\"><li><span><a href=\"#데이터-샘플-시각화-(Show-example-image-and-mask)\" data-toc-modified-id=\"데이터-샘플-시각화-(Show-example-image-and-mask)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>데이터 샘플 시각화 (Show example image and mask)</a></span></li></ul></li><li><span><a href=\"#baseline-model\" data-toc-modified-id=\"baseline-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>baseline model</a></span><ul class=\"toc-item\"><li><span><a href=\"#smp.Unet()\" data-toc-modified-id=\"smp.Unet()-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span><code>smp.Unet()</code></a></span></li></ul></li><li><span><a href=\"#train,-validation,-test-함수-정의\" data-toc-modified-id=\"train,-validation,-test-함수-정의-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>train, validation, test 함수 정의</a></span></li><li><span><a href=\"#모델-저장-함수-정의\" data-toc-modified-id=\"모델-저장-함수-정의-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>모델 저장 함수 정의</a></span></li><li><span><a href=\"#모델-생성-및-Loss-function,-Optimizer-정의\" data-toc-modified-id=\"모델-생성-및-Loss-function,-Optimizer-정의-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>모델 생성 및 Loss function, Optimizer 정의</a></span></li><li><span><a href=\"#저장된-model-불러오기-(학습된-이후)\" data-toc-modified-id=\"저장된-model-불러오기-(학습된-이후)-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>저장된 model 불러오기 (학습된 이후)</a></span><ul class=\"toc-item\"><li><span><a href=\"#plot_examples()-시각화-함수-정의\" data-toc-modified-id=\"plot_examples()-시각화-함수-정의-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span><code>plot_examples()</code> 시각화 함수 정의</a></span><ul class=\"toc-item\"><li><span><a href=\"#train-set-시각화\" data-toc-modified-id=\"train-set-시각화-9.1.1\"><span class=\"toc-item-num\">9.1.1&nbsp;&nbsp;</span>train set 시각화</a></span></li><li><span><a href=\"#validation-set-시각화\" data-toc-modified-id=\"validation-set-시각화-9.1.2\"><span class=\"toc-item-num\">9.1.2&nbsp;&nbsp;</span>validation set 시각화</a></span></li><li><span><a href=\"#test-set-시각화\" data-toc-modified-id=\"test-set-시각화-9.1.3\"><span class=\"toc-item-num\">9.1.3&nbsp;&nbsp;</span>test set 시각화</a></span></li></ul></li></ul></li><li><span><a href=\"#submission을-위한-test-함수-정의\" data-toc-modified-id=\"submission을-위한-test-함수-정의-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>submission을 위한 test 함수 정의</a></span></li><li><span><a href=\"#submission.csv-생성\" data-toc-modified-id=\"submission.csv-생성-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>submission.csv 생성</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T04:02:51.737581Z",
     "start_time": "2021-10-06T04:02:47.421560Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.7.1\n",
      "GPU 사용 가능 여부: True\n",
      "Tesla V100-SXM2-32GB\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import label_accuracy_score, add_hist\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from matplotlib.patches import Patch\n",
    "import webcolors\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "# GPU 사용 가능 여부에 따라 device 정보 저장\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 세팅 및 seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T15:35:17.474108Z",
     "start_time": "2021-10-04T15:35:17.467107Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8   # Mini-batch size\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T15:35:17.923607Z",
     "start_time": "2021-10-04T15:35:17.907607Z"
    }
   },
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 21\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mboostcampcv08\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/boostcampcv08/Naver_Segmentation/runs/1svo478b\" target=\"_blank\">resnet101_unet++</a></strong> to <a href=\"https://wandb.ai/boostcampcv08/Naver_Segmentation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/boostcampcv08/Naver_Segmentation/runs/1svo478b?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fcc18987e50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'baseline_resnet101_unet++.ipynb'\n",
    "wandb.init(project='Naver_Segmentation', name='resnet101_unet++',config={\n",
    "    \"batch size\": batch_size,\n",
    "    \"epochs\" : num_epochs,\n",
    "    \"learning rate\": learning_rate,\n",
    "    \"backborn\" : 'resnet101',\n",
    "    \"architecture\" : 'unet++'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:55:04.002687Z",
     "start_time": "2021-10-04T05:54:59.781190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of super categories: 10\n",
      "Number of categories: 10\n",
      "Number of annotations: 26240\n",
      "Number of images: 3272\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "dataset_path  = '../input/data'\n",
    "anns_file_path = dataset_path + '/' + 'train_all.json'\n",
    "\n",
    "# Read annotations\n",
    "with open(anns_file_path, 'r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "categories = dataset['categories']\n",
    "anns = dataset['annotations']\n",
    "imgs = dataset['images']\n",
    "nr_cats = len(categories)\n",
    "nr_annotations = len(anns)\n",
    "nr_images = len(imgs)\n",
    "\n",
    "# Load categories and super categories\n",
    "cat_names = []\n",
    "super_cat_names = []\n",
    "super_cat_ids = {}\n",
    "super_cat_last_name = ''\n",
    "nr_super_cats = 0\n",
    "for cat_it in categories:\n",
    "    cat_names.append(cat_it['name'])\n",
    "    super_cat_name = cat_it['supercategory']\n",
    "    # Adding new supercat\n",
    "    if super_cat_name != super_cat_last_name:\n",
    "        super_cat_names.append(super_cat_name)\n",
    "        super_cat_ids[super_cat_name] = nr_super_cats\n",
    "        super_cat_last_name = super_cat_name\n",
    "        nr_super_cats += 1\n",
    "\n",
    "print('Number of super categories:', nr_super_cats)\n",
    "print('Number of categories:', nr_cats)\n",
    "print('Number of annotations:', nr_annotations)\n",
    "print('Number of images:', nr_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:55:04.219662Z",
     "start_time": "2021-10-04T05:55:04.004661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count annotations\n",
    "cat_histogram = np.zeros(nr_cats,dtype=int)\n",
    "for ann in anns:\n",
    "    cat_histogram[ann['category_id']-1] += 1\n",
    "\n",
    "# Initialize the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
    "df = df.sort_values('Number of annotations', 0, False)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.title(\"category distribution of train_all set \")\n",
    "plot_1 = sns.barplot(x=\"Number of annotations\", y=\"Categories\", data=df, label=\"Total\", color=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:55:04.235191Z",
     "start_time": "2021-10-04T05:55:04.220662Z"
    }
   },
   "outputs": [],
   "source": [
    "# category labeling \n",
    "sorted_temp_df = df.sort_index()\n",
    "\n",
    "# background = 0 에 해당되는 label 추가 후 기존들을 모두 label + 1 로 설정\n",
    "sorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\n",
    "sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 함수 정의 (Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:16:06.631207Z",
     "start_time": "2021-10-04T06:16:06.620206Z"
    }
   },
   "outputs": [],
   "source": [
    "category_names = list(sorted_df.Categories)\n",
    "\n",
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        images /= 255.0\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # General trash = 1, ... , Cigarette = 10\n",
    "            anns = sorted(anns, key=lambda idx : len(idx['segmentation'][0]), reverse=False)\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks[self.coco.annToMask(anns[i]) == 1] = pixel_value\n",
    "            masks = masks.astype(np.int8)\n",
    "                        \n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            return images, image_infos\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 정의 및 DataLoader 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:16:11.389706Z",
     "start_time": "2021-10-04T06:16:07.146708Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.72s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.94s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train_all.json'\n",
    "val_path = dataset_path + '/val.json'\n",
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "                          ToTensorV2()\n",
    "                          ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                           ToTensorV2()\n",
    "                           ])\n",
    "\n",
    "# create own Dataset 1 (skip)\n",
    "# validation set을 직접 나누고 싶은 경우\n",
    "# random_split 사용하여 data set을 8:2 로 분할\n",
    "# train_size = int(0.8*len(dataset))\n",
    "# val_size = int(len(dataset)-train_size)\n",
    "# dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=transform)\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# create own Dataset 2\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = CustomDataLoader(data_dir=val_path, mode='val', transform=val_transform)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=4,\n",
    "                                         collate_fn=collate_fn)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline model\n",
    "\n",
    "### `smp.UnetPlusPlus()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T04:03:02.944051Z",
     "start_time": "2021-10-06T04:02:57.963052Z"
    }
   },
   "outputs": [],
   "source": [
    "# jupyter command 에서 library download 하기\n",
    "# !pip install git+https://github.com/qubvel/segmentation_models.pytorch\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# model 불러오기\n",
    "# 출력 label 수 정의 (classes=11)\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"resnet101\", # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=11,                     # model output channels (number of classes in your dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T04:03:15.778052Z",
     "start_time": "2021-10-06T04:03:14.388084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape : torch.Size([2, 3, 512, 512])\n",
      "output shape : torch.Size([2, 11, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# 구현된 model에 임의의 input을 넣어 output이 잘 나오는지 test\n",
    "x = torch.randn([2, 3, 512, 512])\n",
    "print(f\"input shape : {x.shape}\")\n",
    "out = model(x)\n",
    "print(f\"output shape : {out.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, validation, test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:27:59.368955Z",
     "start_time": "2021-09-08T08:27:59.351957Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, val_loader, criterion, optimizer, saved_dir, val_every, device):\n",
    "    print(f'Start training..')\n",
    "    n_class = 11\n",
    "    best_loss = 9999999\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        hist = np.zeros((n_class, n_class))\n",
    "        epoch_loss = 0\n",
    "        for step, (images, masks, _) in enumerate(data_loader):\n",
    "            images = torch.stack(images)       \n",
    "            masks = torch.stack(masks).long() \n",
    "            \n",
    "            # gpu 연산을 위해 device 할당\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # device 할당\n",
    "            model = model.to(device)\n",
    "            \n",
    "            # inference\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # loss 계산 (cross entropy loss)\n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "            masks = masks.detach().cpu().numpy()\n",
    "            \n",
    "            hist = add_hist(hist, masks, outputs, n_class=n_class)\n",
    "            acc, acc_cls, mIoU, fwavacc, IoU = label_accuracy_score(hist)\n",
    "            wandb.log({\n",
    "            \"train acc\": acc,\n",
    "            \"train acc cls\" : acc_cls,\n",
    "            \"train mIoU\": mIoU,\n",
    "            \"train fwavacc\": fwavacc,\n",
    "            }) \n",
    "            epoch_loss += loss.item()\n",
    "            # step 주기에 따른 loss 출력\n",
    "            if (step + 1) % 25 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{len(train_loader)}], \\\n",
    "                        Loss: {round(loss.item(),4)}, mIoU: {round(mIoU,4)}')\n",
    "     \n",
    "        # validation 주기에 따른 loss 출력 및 best model 저장\n",
    "        # if (epoch + 1) % val_every == 0:\n",
    "        #     avrg_loss = validation(epoch + 1, model, val_loader, criterion, device)\n",
    "        #     if avrg_loss < best_loss:\n",
    "        #         print(f\"Best performance at epoch: {epoch + 1}\")\n",
    "        #         print(f\"Save model in {saved_dir}\")\n",
    "        #         best_loss = avrg_loss\n",
    "        #         save_model(model, saved_dir)\n",
    "        \n",
    "        # validation없이 전체 데이터로 학습 하기 때문에 train loss기준으로 model 저장\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            avrg_loss = epoch_loss / len(data_loader)\n",
    "            if  avrg_loss < best_loss:\n",
    "                print(f\"Best performance at epoch: {epoch + 1}\")\n",
    "                print(f\"Save model in {saved_dir}\")\n",
    "                best_loss = avrg_loss\n",
    "                save_model(model, saved_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:27:59.631310Z",
     "start_time": "2021-09-08T08:27:59.620809Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, device):\n",
    "    print(f'Start validation #{epoch}')\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_class = 11\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        \n",
    "        hist = np.zeros((n_class, n_class))\n",
    "        for step, (images, masks, _) in enumerate(data_loader):\n",
    "            \n",
    "            images = torch.stack(images)       \n",
    "            masks = torch.stack(masks).long()  \n",
    "\n",
    "            images, masks = images.to(device), masks.to(device)            \n",
    "            \n",
    "            # device 할당\n",
    "            model = model.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "            \n",
    "            outputs = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "            masks = masks.detach().cpu().numpy()\n",
    "            \n",
    "            hist = add_hist(hist, masks, outputs, n_class=n_class)\n",
    "        \n",
    "        acc, acc_cls, mIoU, fwavacc, IoU = label_accuracy_score(hist)\n",
    "        IoU_by_class = [{classes : round(IoU,4)} for IoU, classes in zip(IoU , sorted_df['Categories'])]\n",
    "        wandb.log({\n",
    "        \"val acc\": acc,\n",
    "        \"val acc cls\" : acc_cls,\n",
    "        \"val mIoU\": mIoU,\n",
    "        \"val fwavacc\": fwavacc,\n",
    "        })\n",
    "        avrg_loss = total_loss / cnt\n",
    "        print(f'Validation #{epoch}  Average Loss: {round(avrg_loss.item(), 4)}, Accuracy : {round(acc, 4)}, \\\n",
    "                mIoU: {round(mIoU, 4)}')\n",
    "        print(f'IoU by class : {IoU_by_class}')\n",
    "        \n",
    "    return avrg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:17:31.592162Z",
     "start_time": "2021-10-04T05:17:31.576161Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 저장 함수 정의\n",
    "val_every = 1\n",
    "\n",
    "saved_dir = './saved'\n",
    "if not os.path.isdir(saved_dir):                                                           \n",
    "    os.mkdir(saved_dir)\n",
    "\n",
    "def save_model(model, saved_dir, file_name='resnet101_unetPP_best_model.pt'):\n",
    "    check_point = {'net': model.state_dict()}\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(model, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성 및 Loss function, Optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:17:32.708161Z",
     "start_time": "2021-10-04T05:17:32.695660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer 정의\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:13:57.428660Z",
     "start_time": "2021-10-04T05:13:57.419159Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training..\n",
      "Epoch [1/20], Step [25/409],                         Loss: 1.8604, mIoU: 0.0681\n",
      "Epoch [1/20], Step [50/409],                         Loss: 1.5234, mIoU: 0.1042\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs, model, train_loader, val_loader, criterion, optimizer, saved_dir, val_every, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 저장된 model 불러오기 (학습된 이후) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:17:34.490690Z",
     "start_time": "2021-10-04T05:17:34.339161Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './saved/efficient_unet_best_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14598/3169319899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# best model 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/segmentation/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/segmentation/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/segmentation/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './saved/efficient_unet_best_model.pt'"
     ]
    }
   ],
   "source": [
    "# best model 저장된 경로\n",
    "model_path = './saved/resnet101_unetPP_best_model.pt'\n",
    "\n",
    "# best model 불러오기\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "state_dict = checkpoint.state_dict()\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model = model.to(device)\n",
    "# 추론을 실행하기 전에는 반드시 설정 (batch normalization, dropout 를 평가 모드로 설정)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_examples()` 시각화 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:13:53.321160Z",
     "start_time": "2021-10-04T05:13:53.304161Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_examples(mode=\"train\", batch_id=0, num_examples=batch_size, dataloaer=train_loader):\n",
    "    \"\"\"Visualization of images and masks according to batch size\n",
    "    Args:\n",
    "        mode: train/val/test (str)\n",
    "        batch_id : 0 (int) \n",
    "        num_examples : 1 ~ batch_size(e.g. 8) (int)\n",
    "        dataloaer : data_loader (dataloader) \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # variable for legend\n",
    "    category_and_rgb = [[category, (r,g,b)] for idx, (category, r, g, b) in enumerate(class_colormap.values)]\n",
    "    legend_elements = [Patch(facecolor=webcolors.rgb_to_hex(rgb), \n",
    "                             edgecolor=webcolors.rgb_to_hex(rgb), \n",
    "                             label=category) for category, rgb in category_and_rgb]\n",
    "    \n",
    "    # test / validation set에 대한 시각화\n",
    "    if (mode in ('train', 'val')):\n",
    "        with torch.no_grad():\n",
    "            for index, (imgs, masks, image_infos) in enumerate(dataloaer):\n",
    "                if index == batch_id:\n",
    "                    image_infos = image_infos\n",
    "                    temp_images = imgs\n",
    "                    temp_masks = masks\n",
    "\n",
    "                    model.eval()\n",
    "                    # inference\n",
    "                    outs = model(torch.stack(temp_images).to(device))\n",
    "                    oms = torch.argmax(outs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "        fig, ax = plt.subplots(nrows=num_examples, ncols=3, figsize=(12, 4*num_examples), constrained_layout=True)\n",
    "        fig.tight_layout()\n",
    "        for row_num in range(num_examples):\n",
    "            # Original Image\n",
    "            ax[row_num][0].imshow(temp_images[row_num].permute([1,2,0]))\n",
    "            ax[row_num][0].set_title(f\"Orignal Image : {image_infos[row_num]['file_name']}\")\n",
    "            # Groud Truth\n",
    "            ax[row_num][1].imshow(label_to_color_image(masks[row_num].detach().cpu().numpy()))\n",
    "            ax[row_num][1].set_title(f\"Groud Truth : {image_infos[row_num]['file_name']}\")\n",
    "            # Pred Mask\n",
    "            ax[row_num][2].imshow(label_to_color_image(oms[row_num]))\n",
    "            ax[row_num][2].set_title(f\"Pred Mask : {image_infos[row_num]['file_name']}\")\n",
    "            ax[row_num][2].legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "        plt.show()\n",
    "    \n",
    "    # test set에 대한 시각화\n",
    "    else :\n",
    "        with torch.no_grad():\n",
    "            for index, (imgs, image_infos) in enumerate(dataloaer):\n",
    "                if index == batch_id:\n",
    "                    image_infos = image_infos\n",
    "                    temp_images = imgs\n",
    "\n",
    "                    model.eval()\n",
    "                    \n",
    "                    # inference\n",
    "                    outs = model(torch.stack(temp_images).to(device))['out']\n",
    "                    oms = torch.argmax(outs, dim=1).detach().cpu().numpy()\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "        fig, ax = plt.subplots(nrows=num_examples, ncols=2, figsize=(10, 4*num_examples), constrained_layout=True)\n",
    "\n",
    "        for row_num in range(num_examples):\n",
    "            # Original Image\n",
    "            ax[row_num][0].imshow(temp_images[row_num].permute([1,2,0]))\n",
    "            ax[row_num][0].set_title(f\"Orignal Image : {image_infos[row_num]['file_name']}\")\n",
    "            # Pred Mask\n",
    "            ax[row_num][1].imshow(label_to_color_image(oms[row_num]))\n",
    "            ax[row_num][1].set_title(f\"Pred Mask : {image_infos[row_num]['file_name']}\")\n",
    "            ax[row_num][1].legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train set 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:13:54.065182Z",
     "start_time": "2021-10-04T05:13:54.051662Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_examples(mode=\"train\", batch_id=7, num_examples=4, dataloaer=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation set 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:14:19.516160Z",
     "start_time": "2021-10-04T05:14:18.709160Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_examples(mode=\"val\", batch_id=0, num_examples=4, dataloaer=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test set 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:55:27.950201Z",
     "start_time": "2021-10-04T05:55:21.585687Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_examples(mode=\"test\", batch_id=0, num_examples=8, dataloaer=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission을 위한 test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:16:19.666705Z",
     "start_time": "2021-10-04T06:16:19.657706Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, data_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(size, size)])\n",
    "    print('Start prediction.')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(tqdm(test_loader)):\n",
    "            \n",
    "            # inference (512 x 512)\n",
    "            outs = model(torch.stack(imgs).to(device))\n",
    "            oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            # resize (256 x 256)\n",
    "            temp_mask = []\n",
    "            for img, mask in zip(np.stack(imgs), oms):\n",
    "                transformed = transform(image=img, mask=mask)\n",
    "                mask = transformed['mask']\n",
    "                temp_mask.append(mask)\n",
    "                \n",
    "            oms = np.array(temp_mask)\n",
    "            \n",
    "            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
    "            preds_array = np.vstack((preds_array, oms))\n",
    "            \n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    \n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:19:10.926207Z",
     "start_time": "2021-10-04T06:16:20.313208Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_submisson.csv 열기\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "# test set에 대한 prediction\n",
    "file_names, preds = test(model, test_loader, device)\n",
    "\n",
    "# PredictionString 대입\n",
    "for file_name, string in zip(file_names, preds):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "submission.to_csv(\"./submission/efficient_unet_best_model.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "segmentation",
   "language": "python",
   "name": "segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "394.25px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
